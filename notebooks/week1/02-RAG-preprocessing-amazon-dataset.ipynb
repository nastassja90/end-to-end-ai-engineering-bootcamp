{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dbac4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "\n",
    "import pandas as pd\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbe9b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset we developed in the previous notebook\n",
    "df_items = pd.read_json(\n",
    "    \"../../data/meta_Electronics_2022_2023_with_category_ratings_100_sample_1000.jsonl\",\n",
    "    lines=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db8f7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dataset raw data\n",
    "df_items.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3596ad2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make some preprocessing to prepare this dataset for the embedding into the vector database\n",
    "# we will add into the description field the concatenation of title + features and we will place\n",
    "# into the image field the first image URL available\n",
    "\n",
    "def preprocess_description(row):\n",
    "    return f\"{row['title']} {' '.join(row['features'])}\"\n",
    "\n",
    "\n",
    "def extract_first_large_image(row):\n",
    "    return row[\"images\"][0].get(\"large\", \"\")\n",
    "\n",
    "df_items[\"description\"] = df_items.apply(preprocess_description, axis=1)\n",
    "df_items[\"image\"] = df_items.apply(extract_first_large_image, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087b2e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the preprocessed dataset\n",
    "df_items.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dc470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example of preprocessed description\n",
    "list(df_items[\"description\"].items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7d2590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now sample just 50 items from the dataset for faster development processing\n",
    "df_sample = df_items.sample(50, random_state=42)\n",
    "# and define just a subset of the fields to be stored in the vector database\n",
    "data_to_embed = df_sample[\n",
    "    [\"description\", \"image\", \"rating_number\", \"price\", \"average_rating\", \"parent_asin\"]\n",
    "].to_dict(orient=\"records\")\n",
    "# Show example of data to be embedded\n",
    "data_to_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86317e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the embedding model (OpenAI) to use\n",
    "model = \"text-embedding-3-small\"\n",
    "# And the embedding function\n",
    "def get_embedding(text, model=model):\n",
    "    response = openai.embeddings.create(\n",
    "        input=text,\n",
    "        model=model,\n",
    "    )\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df43baa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before proceeding let's make a test to retrieve the size of the embedding produced \n",
    "# by the model we defined. This is important because when we create\n",
    "# Qdrant client collection we need to specify the size of the vectors to be stored\n",
    "test_embedding = get_embedding(\"This is a test embedding\")\n",
    "len(test_embedding)  # Should be 1536 for text-embedding-3-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7a584f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Qdrant client (ensure first to have Qdrant server running locally via docker compose; \n",
    "# run the command `make run-docker-compose` in the root directory, then execute this cell)\n",
    "qdrant_client = QdrantClient(url=\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb495380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Qdrant collection to store the data_to_embed items\n",
    "collection_name = \"Amazon-items-collection-00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b278f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Qdrant collection to store the Amazon items\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f122cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed th dataset data (data_to_embed) and store into Qdrant collection\n",
    "pointstructs = []\n",
    "for i, data in enumerate(data_to_embed):\n",
    "    pointstructs.append(\n",
    "        PointStruct(\n",
    "            id=i,\n",
    "            # we create a vector embedding for the description field\n",
    "            vector=get_embedding(data[\"description\"]),\n",
    "            # we store all the data fields as payload metadata in Qdrant\n",
    "            payload=data,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8eafbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the pointstructs to be uploaded\n",
    "pointstructs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71fddd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert the points into Qdrant collection\n",
    "qdrant_client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    wait=True,\n",
    "    points=pointstructs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d934c8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function for data retrieval from Qdrant based on a query text\n",
    "# top K similar items will be retrieved\n",
    "def retrieve_data(query, k=5):\n",
    "    results = qdrant_client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        # Notice that we are using the same embedding function to convert the query text into a vector\n",
    "        query=get_embedding(query),\n",
    "        limit=k,\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91371136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally test the retrieval function\n",
    "retrieve_data(\"What kind of charging cords do you offer?\", k=10).points"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
