{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cfa5d2c",
   "metadata": {},
   "source": [
    "In this example, we use instructor to enrich the response with the actual data of each reference ID that was returned as the top K elements from the retrieving search in Qdrant database. The code is basically the same of the previous notebook, except for the fact that StructuredResponse now contains additional fields to better model the response we want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fce2be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import instructor\n",
    "from qdrant_client import QdrantClient\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b408ad2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pydantic model for the ReferencedItem retrieved from the vector database.\n",
    "class ReferencedItem(BaseModel):\n",
    "    id: str = Field(..., description=\"The unique identifier of the referenced item (parent ASIN).\")\n",
    "    description: str = Field(..., description=\"The short description of the referenced item.\")\n",
    "\n",
    "# Define the output schema using Pydantic. This schema will be used to structure the model's response via instructor.\n",
    "class StructuredResponse(BaseModel):\n",
    "    answer: str = Field(\n",
    "        ..., description=\"A brief summary of the weather in Italy today.\"\n",
    "    )\n",
    "    references: list[ReferencedItem] = Field(..., description=\"A list of items used to answer the question.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2a8e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the instructor client for the openai client\n",
    "client = instructor.from_openai(openai.OpenAI())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458f814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's define a sample RAG pipeline\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    response = openai.embeddings.create(\n",
    "        input=text,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    return response.data[0].embedding\n",
    "\n",
    "\n",
    "def retrieve_data(query, qdrant_client, k=5):\n",
    "\n",
    "    query_embedding = get_embedding(query)\n",
    "\n",
    "    results = qdrant_client.query_points(\n",
    "        collection_name=\"Amazon-items-collection-00\",\n",
    "        query=query_embedding,\n",
    "        limit=k,\n",
    "    )\n",
    "\n",
    "    retrieved_context_ids = []\n",
    "    retrieved_context = []\n",
    "    similarity_scores = []\n",
    "    retrieved_context_ratings = []\n",
    "\n",
    "    for result in results.points:\n",
    "        retrieved_context_ids.append(result.payload[\"parent_asin\"])\n",
    "        retrieved_context.append(result.payload[\"description\"])\n",
    "        retrieved_context_ratings.append(result.payload[\"average_rating\"])\n",
    "        similarity_scores.append(result.score)\n",
    "\n",
    "    return {\n",
    "        \"retrieved_context_ids\": retrieved_context_ids,\n",
    "        \"retrieved_context\": retrieved_context,\n",
    "        \"retrieved_context_ratings\": retrieved_context_ratings,\n",
    "        \"similarity_scores\": similarity_scores,\n",
    "    }\n",
    "\n",
    "\n",
    "def process_context(context):\n",
    "\n",
    "    formatted_context = \"\"\n",
    "\n",
    "    for id, chunk, rating in zip(\n",
    "        context[\"retrieved_context_ids\"],\n",
    "        context[\"retrieved_context\"],\n",
    "        context[\"retrieved_context_ratings\"],\n",
    "    ):\n",
    "        formatted_context += f\"- ID: {id}, rating: {rating}, description: {chunk}\\n\"\n",
    "\n",
    "    return formatted_context\n",
    "\n",
    "\n",
    "def build_prompt(preprocessed_context, question):\n",
    "    # Let's use a more specific prompt for the shopping assistant;\n",
    "    # now we instruct explicitly the LLM about how to structure the output answer.\n",
    "    # Instructor enforces a Pydantic schema on LLM outputs, but the schema fields don't need to exist\n",
    "    # in the source data. The LLM can generate fields dynamically (e.g., summaries, transformations)\n",
    "    # based on the context you provide in the prompt. Instructor handles schema validation and retries,\n",
    "    # but you must explicitly instruct the LLM on how to generate each field.\n",
    "\n",
    "    # In our example the description field in the ReferencedItem model is not present in the source data,\n",
    "    # so we need to instruct the LLM to generate it based on the description provided in the context (the prompt below).\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a shopping assistant that can answer questions about the products in stock.\n",
    "\n",
    "You will be given a question and a list of context.\n",
    "\n",
    "Instructions:\n",
    "- You need to answer the question based on the provided context only.\n",
    "- Never use word context and refer to it as the available products.\n",
    "- As an output you need to provide:\n",
    "\n",
    "* The answer to the question based on the provided context.\n",
    "* The list of the IDs that were used to answer the question. Only return the ones used in the answer.\n",
    "* Short description (1-2 sentences) of each item based on the description provided in the context.\n",
    "\n",
    "- The short description should have the name of the item.\n",
    "- The answer to the question should contain detailed information about the product and return with detailed specifications in bullet points.\n",
    "\n",
    "Context:\n",
    "{preprocessed_context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Instead of using the Openai client, let's use the instructor client here as well\n",
    "# and the StructuredResponse model defined above\n",
    "def generate_answer(prompt):\n",
    "\n",
    "    response, original_response = client.chat.completions.create_with_completion(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "        temperature=0,\n",
    "        response_model=StructuredResponse,\n",
    "    )\n",
    "    # the return object is the StructuredResponse model instance\n",
    "    return response\n",
    "\n",
    "\n",
    "def rag_pipeline(question, qdrant_client, top_k=5):\n",
    "\n",
    "    retrieved_context = retrieve_data(question, qdrant_client, top_k)\n",
    "    preprocessed_context = process_context(retrieved_context)\n",
    "    prompt = build_prompt(preprocessed_context, question)\n",
    "    response = generate_answer(prompt)\n",
    "\n",
    "    final_result = {\n",
    "        # print the full data model response for reference (debugging/tracing)\n",
    "        \"data_model\": response,\n",
    "        \"answer\": response.answer,\n",
    "        # add the references as well\n",
    "        \"references\": response.references,\n",
    "        \"question\": question,\n",
    "        \"retrieved_context_ids\": retrieved_context[\"retrieved_context_ids\"],\n",
    "        \"retrieved_context\": retrieved_context[\"retrieved_context\"],\n",
    "        \"retrieved_context_ratings\": retrieved_context[\"retrieved_context_ratings\"],\n",
    "        \"similarity_scores\": retrieved_context[\"similarity_scores\"],\n",
    "    }\n",
    "\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea63ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Qdrant client (ensure to start the container first)\n",
    "qdrant_client = QdrantClient(url=\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6878e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's invoke the rag pipeline\n",
    "query = \"What are some good products with high ratings for outdoor activities?\"\n",
    "# let's increase the top_key to 10 to test that the prompt actually returns only the relevant items\n",
    "# not necessarily all the top 10 items.\n",
    "output = rag_pipeline(query, qdrant_client, top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35cdb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5423d7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
